"""
é«˜çº§æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å—
æä¾›æ›´è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®
"""

import torch
import time
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
from .performance_benchmark import TritonPerformanceBenchmark

logger = logging.getLogger(__name__)


class AdvancedTritonBenchmark(TritonPerformanceBenchmark):
    """é«˜çº§Tritonæ€§èƒ½åŸºå‡†æµ‹è¯•å™¨ï¼Œæä¾›è¯¦ç»†åˆ†æ"""

    def __init__(self, warmup_runs: int = 10, benchmark_runs: int = 100):
        super().__init__(warmup_runs, benchmark_runs)
        self.detailed_results = {}

    def run_comprehensive_analysis(self, kernel_file_path: str, test_file_path: str) -> Dict[str, Any]:
        """
        è¿è¡Œå…¨é¢çš„æ€§èƒ½åˆ†æ
        
        Args:
            kernel_file_path: å†…æ ¸æ–‡ä»¶è·¯å¾„
            test_file_path: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            
        Returns:
            è¯¦ç»†çš„æ€§èƒ½åˆ†æç»“æœ
        """
        results = {
            "basic_performance": None,
            "memory_analysis": None,
            "scaling_analysis": None,
            "optimization_suggestions": [],
            "performance_grade": None
        }
        
        try:
            # 1. åŸºç¡€æ€§èƒ½æµ‹è¯•
            logger.info("ğŸ” è¿è¡ŒåŸºç¡€æ€§èƒ½æµ‹è¯•...")
            basic_perf = self.benchmark_kernel_from_file(kernel_file_path, test_file_path)
            results["basic_performance"] = basic_perf
            
            if not basic_perf["success"]:
                return results
            
            # 2. å†…å­˜åˆ†æ
            logger.info("ğŸ“Š åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼...")
            memory_analysis = self._analyze_memory_patterns(kernel_file_path, test_file_path)
            results["memory_analysis"] = memory_analysis
            
            # 3. æ‰©å±•æ€§åˆ†æ
            logger.info("ğŸ“ˆ åˆ†ææ€§èƒ½æ‰©å±•æ€§...")
            scaling_analysis = self._analyze_scaling_performance(kernel_file_path, test_file_path)
            results["scaling_analysis"] = scaling_analysis
            
            # 4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
            logger.info("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
            suggestions = self._generate_optimization_suggestions(basic_perf, memory_analysis, scaling_analysis)
            results["optimization_suggestions"] = suggestions
            
            # 5. ç»¼åˆè¯„çº§
            grade = self._calculate_comprehensive_grade(basic_perf, memory_analysis, scaling_analysis)
            results["performance_grade"] = grade
            
            logger.info("âœ… é«˜çº§æ€§èƒ½åˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"é«˜çº§æ€§èƒ½åˆ†æå¤±è´¥: {e}")
            results["error"] = str(e)
        
        return results

    def _analyze_memory_patterns(self, kernel_file_path: str, test_file_path: str) -> Dict[str, Any]:
        """åˆ†æå†…å­˜è®¿é—®æ¨¡å¼"""
        try:
            # åŠ è½½å†…æ ¸å’Œæµ‹è¯•
            kernel_func = self._load_kernel_function(kernel_file_path)
            test_inputs, _ = self._extract_test_info(test_file_path)
            
            if not kernel_func or not test_inputs:
                return {"success": False, "error": "æ— æ³•åŠ è½½å†…æ ¸æˆ–æµ‹è¯•"}
            
            # æµ‹è¯•ä¸åŒå†…å­˜å¸ƒå±€çš„æ€§èƒ½
            memory_results = {
                "success": True,
                "contiguous_performance": None,
                "strided_performance": None,
                "memory_efficiency": None,
                "bandwidth_utilization": None
            }
            
            # è¿ç»­å†…å­˜æµ‹è¯•
            contiguous_input = test_inputs[0].contiguous()
            contiguous_time = self._benchmark_triton(kernel_func, [contiguous_input])
            memory_results["contiguous_performance"] = contiguous_time
            
            # æ­¥é•¿å†…å­˜æµ‹è¯•ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                strided_input = test_inputs[0][::2].contiguous()  # æ¯éš”ä¸€ä¸ªå…ƒç´ 
                strided_time = self._benchmark_triton(kernel_func, [strided_input])
                memory_results["strided_performance"] = strided_time
            except:
                memory_results["strided_performance"] = None
            
            # è®¡ç®—å†…å­˜æ•ˆç‡
            tensor_size_bytes = test_inputs[0].numel() * test_inputs[0].element_size()
            if contiguous_time and contiguous_time > 0:
                # å‡è®¾è¯»å†™å„ä¸€æ¬¡
                bandwidth_gb_s = (2 * tensor_size_bytes / (1024**3)) / (contiguous_time / 1000)
                memory_results["bandwidth_utilization"] = bandwidth_gb_s
                
                # å†…å­˜æ•ˆç‡è¯„åˆ†ï¼ˆç›¸å¯¹äºç†è®ºå³°å€¼ï¼‰
                theoretical_bandwidth = 900  # GB/s for modern GPUs
                efficiency = min(bandwidth_gb_s / theoretical_bandwidth * 100, 100)
                memory_results["memory_efficiency"] = efficiency
            
            return memory_results
            
        except Exception as e:
            logger.error(f"å†…å­˜åˆ†æå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    def _analyze_scaling_performance(self, kernel_file_path: str, test_file_path: str) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ‰©å±•æ€§"""
        try:
            kernel_func = self._load_kernel_function(kernel_file_path)
            if not kernel_func:
                return {"success": False, "error": "æ— æ³•åŠ è½½å†…æ ¸"}
            
            scaling_results = {
                "success": True,
                "size_performance": [],
                "scaling_efficiency": None,
                "optimal_size": None
            }
            
            # æµ‹è¯•ä¸åŒå¤§å°çš„æ€§èƒ½
            test_sizes = [1024, 4096, 16384, 65536, 262144]
            
            for size in test_sizes:
                try:
                    # åˆ›å»ºæµ‹è¯•è¾“å…¥
                    test_input = torch.randn(size, dtype=torch.float32, device='cuda')
                    
                    # æµ‹è¯•æ€§èƒ½
                    triton_time = self._benchmark_triton(kernel_func, [test_input])
                    pytorch_time = self._benchmark_pytorch(lambda x: torch.relu(x), [test_input])
                    
                    if triton_time and pytorch_time:
                        speedup = pytorch_time / triton_time
                        throughput = size / (triton_time / 1000)  # elements per second
                        
                        scaling_results["size_performance"].append({
                            "size": size,
                            "triton_time_ms": triton_time,
                            "pytorch_time_ms": pytorch_time,
                            "speedup": speedup,
                            "throughput_elements_per_sec": throughput
                        })
                except Exception as e:
                    logger.debug(f"è·³è¿‡å¤§å° {size}: {e}")
                    continue
            
            # åˆ†ææ‰©å±•æ•ˆç‡
            if len(scaling_results["size_performance"]) >= 2:
                perfs = scaling_results["size_performance"]
                
                # æ‰¾åˆ°æœ€ä½³æ€§èƒ½ç‚¹
                best_speedup = max(p["speedup"] for p in perfs)
                optimal_size = next(p["size"] for p in perfs if p["speedup"] == best_speedup)
                scaling_results["optimal_size"] = optimal_size
                
                # è®¡ç®—æ‰©å±•æ•ˆç‡ï¼ˆç†æƒ³æƒ…å†µä¸‹ååé‡åº”è¯¥éšå¤§å°çº¿æ€§å¢é•¿ï¼‰
                throughputs = [p["throughput_elements_per_sec"] for p in perfs]
                sizes = [p["size"] for p in perfs]
                
                # ç®€å•çš„çº¿æ€§æ‹Ÿåˆæ¥è¯„ä¼°æ‰©å±•æ€§
                if len(throughputs) >= 2:
                    correlation = np.corrcoef(sizes, throughputs)[0, 1]
                    scaling_results["scaling_efficiency"] = max(0, correlation * 100)
            
            return scaling_results
            
        except Exception as e:
            logger.error(f"æ‰©å±•æ€§åˆ†æå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    def _generate_optimization_suggestions(self, basic_perf: Dict, memory_analysis: Dict, scaling_analysis: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        try:
            # åŸºäºåŸºç¡€æ€§èƒ½çš„å»ºè®®
            if basic_perf and basic_perf["success"]:
                speedup = basic_perf["speedup"]
                
                if speedup < 0.8:
                    suggestions.append("ğŸ”§ å†…æ ¸æ€§èƒ½ä½äºPyTorchï¼Œè€ƒè™‘ä¼˜åŒ–ç®—æ³•æˆ–å¢åŠ å¹¶è¡Œåº¦")
                elif speedup < 1.2:
                    suggestions.append("âš¡ æ€§èƒ½æ¥è¿‘PyTorchï¼Œå¯å°è¯•è°ƒæ•´block sizeæˆ–å†…å­˜è®¿é—®æ¨¡å¼")
                elif speedup > 2.0:
                    suggestions.append("ğŸ† æ€§èƒ½ä¼˜ç§€ï¼å¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥è¾¾åˆ°æ›´é«˜åŠ é€Ÿæ¯”")
            
            # åŸºäºå†…å­˜åˆ†æçš„å»ºè®®
            if memory_analysis and memory_analysis["success"]:
                if memory_analysis.get("memory_efficiency"):
                    efficiency = memory_analysis["memory_efficiency"]
                    if efficiency < 30:
                        suggestions.append("ğŸ’¾ å†…å­˜å¸¦å®½åˆ©ç”¨ç‡è¾ƒä½ï¼Œæ£€æŸ¥å†…å­˜è®¿é—®æ¨¡å¼å’Œåˆå¹¶")
                    elif efficiency < 60:
                        suggestions.append("ğŸ“ˆ å†…å­˜æ•ˆç‡ä¸­ç­‰ï¼Œå¯ä¼˜åŒ–æ•°æ®å¸ƒå±€æˆ–è®¿é—®æ¨¡å¼")
                
                # æ¯”è¾ƒè¿ç»­å’Œæ­¥é•¿æ€§èƒ½
                cont_perf = memory_analysis.get("contiguous_performance")
                stride_perf = memory_analysis.get("strided_performance")
                if cont_perf and stride_perf and stride_perf > cont_perf * 1.5:
                    suggestions.append("ğŸ”„ æ­¥é•¿è®¿é—®æ€§èƒ½è¾ƒå·®ï¼Œç¡®ä¿æ•°æ®è¿ç»­æ€§")
            
            # åŸºäºæ‰©å±•æ€§åˆ†æçš„å»ºè®®
            if scaling_analysis and scaling_analysis["success"]:
                scaling_eff = scaling_analysis.get("scaling_efficiency")
                if scaling_eff and scaling_eff < 70:
                    suggestions.append("ğŸ“Š æ‰©å±•æ€§è¾ƒå·®ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨æ€§èƒ½ç“¶é¢ˆæˆ–åŒæ­¥é—®é¢˜")
                
                optimal_size = scaling_analysis.get("optimal_size")
                if optimal_size:
                    suggestions.append(f"ğŸ¯ æœ€ä½³æ€§èƒ½å‡ºç°åœ¨å¤§å° {optimal_size}ï¼Œè€ƒè™‘é’ˆå¯¹æ­¤å¤§å°ä¼˜åŒ–")
            
            # é€šç”¨å»ºè®®
            if not suggestions:
                suggestions.append("âœ¨ æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå¯è€ƒè™‘æµ‹è¯•æ›´å¤§çš„æ•°æ®é›†æˆ–å¤æ‚åœºæ™¯")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå»ºè®®å¤±è´¥: {e}")
            suggestions.append("â“ æ— æ³•ç”Ÿæˆå…·ä½“å»ºè®®ï¼Œè¯·æ£€æŸ¥æ€§èƒ½æ•°æ®")
        
        return suggestions

    def _calculate_comprehensive_grade(self, basic_perf: Dict, memory_analysis: Dict, scaling_analysis: Dict) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆæ€§èƒ½è¯„çº§"""
        try:
            grade_info = {
                "overall_score": 0,
                "performance_score": 0,
                "memory_score": 0,
                "scaling_score": 0,
                "grade": "æœªçŸ¥",
                "details": {}
            }
            
            scores = []
            
            # åŸºç¡€æ€§èƒ½è¯„åˆ† (40%)
            if basic_perf and basic_perf["success"]:
                speedup = basic_perf["speedup"]
                if speedup >= 2.0:
                    perf_score = 100
                elif speedup >= 1.5:
                    perf_score = 85
                elif speedup >= 1.0:
                    perf_score = 70
                elif speedup >= 0.8:
                    perf_score = 55
                else:
                    perf_score = 30
                
                grade_info["performance_score"] = perf_score
                scores.append(("performance", perf_score, 0.4))
            
            # å†…å­˜æ•ˆç‡è¯„åˆ† (30%)
            if memory_analysis and memory_analysis["success"]:
                mem_eff = memory_analysis.get("memory_efficiency", 50)
                mem_score = min(mem_eff * 2, 100)  # è½¬æ¢ä¸º0-100åˆ†
                
                grade_info["memory_score"] = mem_score
                scores.append(("memory", mem_score, 0.3))
            
            # æ‰©å±•æ€§è¯„åˆ† (30%)
            if scaling_analysis and scaling_analysis["success"]:
                scale_eff = scaling_analysis.get("scaling_efficiency", 50)
                scale_score = min(scale_eff, 100)
                
                grade_info["scaling_score"] = scale_score
                scores.append(("scaling", scale_score, 0.3))
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            if scores:
                total_weight = sum(weight for _, _, weight in scores)
                weighted_sum = sum(score * weight for _, score, weight in scores)
                overall_score = weighted_sum / total_weight
                
                grade_info["overall_score"] = overall_score
                
                # ç¡®å®šç­‰çº§
                if overall_score >= 90:
                    grade_info["grade"] = "ä¼˜ç§€ ğŸ†"
                elif overall_score >= 80:
                    grade_info["grade"] = "è‰¯å¥½ âœ…"
                elif overall_score >= 70:
                    grade_info["grade"] = "ä¸­ç­‰ âš¡"
                elif overall_score >= 60:
                    grade_info["grade"] = "ä¸€èˆ¬ âš ï¸"
                else:
                    grade_info["grade"] = "éœ€ä¼˜åŒ– âŒ"
                
                grade_info["details"] = {
                    "performance_weight": "40%",
                    "memory_weight": "30%", 
                    "scaling_weight": "30%",
                    "total_components": len(scores)
                }
            
            return grade_info
            
        except Exception as e:
            logger.error(f"è®¡ç®—è¯„çº§å¤±è´¥: {e}")
            return {"overall_score": 0, "grade": "é”™è¯¯ âŒ", "error": str(e)}

    def generate_performance_report(self, results: Dict[str, Any], output_path: Optional[str] = None) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š"""
        try:
            report_lines = []
            report_lines.append("=" * 80)
            report_lines.append("ğŸš€ TRITON å†…æ ¸æ€§èƒ½åˆ†ææŠ¥å‘Š")
            report_lines.append("=" * 80)
            
            # åŸºç¡€æ€§èƒ½
            if results.get("basic_performance"):
                basic = results["basic_performance"]
                if basic["success"]:
                    report_lines.append("\nğŸ“Š åŸºç¡€æ€§èƒ½æŒ‡æ ‡:")
                    report_lines.append(f"   PyTorchæ—¶é—´: {basic['pytorch_time_ms']:.3f}ms")
                    report_lines.append(f"   Tritonæ—¶é—´:  {basic['triton_time_ms']:.3f}ms")
                    report_lines.append(f"   åŠ é€Ÿæ¯”:      {basic['speedup']:.2f}x")
                    # if basic.get("memory_usage_mb"):
                    #     report_lines.append(f"   å†…å­˜ä½¿ç”¨:    {basic['memory_usage_mb']:.1f}MB")
            
            # å†…å­˜åˆ†æ
            if results.get("memory_analysis"):
                mem = results["memory_analysis"]
                if mem["success"]:
                    report_lines.append("\nğŸ’¾ å†…å­˜åˆ†æ:")
                    if mem.get("memory_efficiency"):
                        report_lines.append(f"   å†…å­˜æ•ˆç‡:    {mem['memory_efficiency']:.1f}%")
                    if mem.get("bandwidth_utilization"):
                        report_lines.append(f"   å¸¦å®½åˆ©ç”¨:    {mem['bandwidth_utilization']:.1f} GB/s")
                    if mem.get("contiguous_performance") and mem.get("strided_performance"):
                        ratio = mem["strided_performance"] / mem["contiguous_performance"]
                        report_lines.append(f"   æ­¥é•¿æ€§èƒ½æ¯”:  {ratio:.2f}x")
            
            # æ‰©å±•æ€§åˆ†æ
            if results.get("scaling_analysis"):
                scale = results["scaling_analysis"]
                if scale["success"]:
                    report_lines.append("\nğŸ“ˆ æ‰©å±•æ€§åˆ†æ:")
                    if scale.get("scaling_efficiency"):
                        report_lines.append(f"   æ‰©å±•æ•ˆç‡:    {scale['scaling_efficiency']:.1f}%")
                    if scale.get("optimal_size"):
                        report_lines.append(f"   æœ€ä½³å¤§å°:    {scale['optimal_size']} å…ƒç´ ")
                    
                    # æ€§èƒ½æ•°æ®è¡¨æ ¼
                    if scale.get("size_performance"):
                        report_lines.append("\n   æ€§èƒ½æ•°æ®:")
                        report_lines.append("   å¤§å°      Triton(ms)  PyTorch(ms)  åŠ é€Ÿæ¯”")
                        report_lines.append("   " + "-" * 45)
                        for perf in scale["size_performance"]:
                            report_lines.append(
                                f"   {perf['size']:8d}  {perf['triton_time_ms']:8.3f}  "
                                f"{perf['pytorch_time_ms']:9.3f}  {perf['speedup']:6.2f}x"
                            )
            
            # ç»¼åˆè¯„çº§
            if results.get("performance_grade"):
                grade = results["performance_grade"]
                report_lines.append("\nğŸ† ç»¼åˆè¯„çº§:")
                report_lines.append(f"   æ€»ä½“è¯„åˆ†:    {grade['overall_score']:.1f}/100")
                report_lines.append(f"   æ€§èƒ½è¯„åˆ†:    {grade['performance_score']:.1f}/100")
                report_lines.append(f"   å†…å­˜è¯„åˆ†:    {grade['memory_score']:.1f}/100")
                report_lines.append(f"   æ‰©å±•è¯„åˆ†:    {grade['scaling_score']:.1f}/100")
                report_lines.append(f"   æœ€ç»ˆç­‰çº§:    {grade['grade']}")
            
            # ä¼˜åŒ–å»ºè®®
            if results.get("optimization_suggestions"):
                report_lines.append("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for i, suggestion in enumerate(results["optimization_suggestions"], 1):
                    report_lines.append(f"   {i}. {suggestion}")
            
            report_lines.append("\n" + "=" * 80)
            
            report_text = "\n".join(report_lines)
            
            # ä¿å­˜æŠ¥å‘Š
            if output_path:
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_text)
                logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            
            return report_text
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}"


def run_advanced_benchmark(session_dir: str) -> Optional[Dict[str, Any]]:
    """
    è¿è¡Œé«˜çº§æ€§èƒ½åŸºå‡†æµ‹è¯•
    
    Args:
        session_dir: ä¼šè¯ç›®å½•è·¯å¾„
        
    Returns:
        é«˜çº§æ€§èƒ½åˆ†æç»“æœ
    """
    try:
        session_path = Path(session_dir)
        kernel_file = session_path / "final_kernel.py"
        test_file = session_path / "test.py"
        
        if not kernel_file.exists() or not test_file.exists():
            logger.warning("ç¼ºå°‘å¿…è¦çš„æ–‡ä»¶è¿›è¡Œé«˜çº§åˆ†æ")
            return None
        
        # åˆ›å»ºé«˜çº§åŸºå‡†æµ‹è¯•å™¨
        benchmark = AdvancedTritonBenchmark(warmup_runs=5, benchmark_runs=30)
        
        # è¿è¡Œå…¨é¢åˆ†æ
        results = benchmark.run_comprehensive_analysis(str(kernel_file), str(test_file))
        
        # ç”ŸæˆæŠ¥å‘Š
        report = benchmark.generate_performance_report(results)
        
        # ä¿å­˜ç»“æœå’ŒæŠ¥å‘Š
        if results:
            with open(session_path / "advanced_performance.json", 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2)
            
            with open(session_path / "performance_report.txt", 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info("é«˜çº§æ€§èƒ½åˆ†æå®Œæˆ")
            logger.info("\n" + report)
        
        return results
        
    except Exception as e:
        logger.error(f"é«˜çº§æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        return None