"""
Tritonå†…æ ¸æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å—
ä¸“é—¨ç”¨äºéªŒè¯æˆåŠŸçš„ç®—å­è¿›è¡Œæ€§èƒ½æµ‹è¯•
"""

import torch
import time
import logging
import tempfile
import os
import sys
import importlib.util
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)


class TritonPerformanceBenchmark:
    """Tritonå†…æ ¸æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self, warmup_runs: int = 5, benchmark_runs: int = 50):
        """
        åˆå§‹åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨
        
        Args:
            warmup_runs: é¢„çƒ­è¿è¡Œæ¬¡æ•°
            benchmark_runs: åŸºå‡†æµ‹è¯•è¿è¡Œæ¬¡æ•°
        """
        self.warmup_runs = warmup_runs
        self.benchmark_runs = benchmark_runs
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        if self.device == "cuda":
            # è®°å½•åŸå§‹TF32è®¾ç½®
            self.original_tf32_matmul = torch.backends.cuda.matmul.allow_tf32
            self.original_tf32_cudnn = torch.backends.cudnn.allow_tf32

    def benchmark_kernel_from_file(self, kernel_file_path: str, test_file_path: str) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶è·¯å¾„åŠ è½½å¹¶æµ‹è¯•å†…æ ¸æ€§èƒ½
        
        Args:
            kernel_file_path: å†…æ ¸æ–‡ä»¶è·¯å¾„ (kernel.py)
            test_file_path: æµ‹è¯•æ–‡ä»¶è·¯å¾„ (test.py)
            
        Returns:
            æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        results = {
            "success": False,
            "error": None,
            "pytorch_time_ms": None,
            "triton_time_ms": None,
            "speedup": None,
            "memory_usage_mb": None
        }
        
        try:
            # 1. åŠ è½½å†…æ ¸å‡½æ•°
            kernel_func = self._load_kernel_function(kernel_file_path)
            if kernel_func is None:
                results["error"] = "æ— æ³•åŠ è½½å†…æ ¸å‡½æ•°"
                return results
            
            # 2. ä»æµ‹è¯•æ–‡ä»¶ä¸­æå–æµ‹è¯•è¾“å…¥
            test_inputs, pytorch_reference = self._extract_test_info(test_file_path)
            if test_inputs is None:
                results["error"] = "æ— æ³•æå–æµ‹è¯•è¾“å…¥"
                return results
            
            # 3. è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
            perf_results = self._run_benchmark(kernel_func, pytorch_reference, test_inputs)
            results.update(perf_results)
            
            if results["success"]:
                logger.info(f"ğŸš€ æ€§èƒ½æµ‹è¯•å®Œæˆ")
                logger.info(f"   PyTorch: {results['pytorch_time_ms']:.3f}ms")
                logger.info(f"   Triton:  {results['triton_time_ms']:.3f}ms")
                logger.info(f"   åŠ é€Ÿæ¯”:  {results['speedup']:.2f}x")
            
        except Exception as e:
            results["error"] = str(e)
            logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        return results

    def _load_kernel_function(self, kernel_file_path: str) -> Optional[Any]:
        """
        ä»kernel.pyæ–‡ä»¶åŠ è½½kernel_function
        
        Args:
            kernel_file_path: kernel.pyæ–‡ä»¶è·¯å¾„
            
        Returns:
            kernel_functionå‡½æ•°å¯¹è±¡
        """
        try:
            # è¯»å–å†…æ ¸ä»£ç 
            with open(kernel_file_path, 'r', encoding='utf-8') as f:
                kernel_code = f.read()
            
            # åˆ›å»ºä¸´æ—¶æ¨¡å—
            spec = importlib.util.spec_from_file_location("kernel_module", kernel_file_path)
            module = importlib.util.module_from_spec(spec)
            
            # æ‰§è¡Œæ¨¡å—
            spec.loader.exec_module(module)
            
            # æŸ¥æ‰¾kernel_function
            if hasattr(module, 'kernel_function'):
                return module.kernel_function
            else:
                logger.error("kernel.pyä¸­æœªæ‰¾åˆ°kernel_functionå‡½æ•°")
                return None
                
        except Exception as e:
            logger.error(f"åŠ è½½å†…æ ¸å‡½æ•°å¤±è´¥: {e}")
            return None

    def _extract_test_info(self, test_file_path: str) -> Tuple[Optional[List[torch.Tensor]], Optional[Any]]:
        """
        ä»test.pyæ–‡ä»¶ä¸­æå–æµ‹è¯•è¾“å…¥å’ŒPyTorchå‚è€ƒå®ç°
        
        Args:
            test_file_path: test.pyæ–‡ä»¶è·¯å¾„
            
        Returns:
            (æµ‹è¯•è¾“å…¥åˆ—è¡¨, PyTorchå‚è€ƒå‡½æ•°)
        """
        try:
            # è¯»å–æµ‹è¯•ä»£ç 
            with open(test_file_path, 'r', encoding='utf-8') as f:
                test_code = f.read()
            
            # æ‰§è¡Œæµ‹è¯•ä»£ç è·å–è¾“å…¥
            namespace = {}
            exec(test_code, namespace)
            
            # æŸ¥æ‰¾æµ‹è¯•è¾“å…¥åˆ›å»ºé€»è¾‘
            test_inputs = self._create_test_inputs_from_code(test_code)
            pytorch_ref = self._create_pytorch_reference_from_code(test_code, test_inputs)
            
            return test_inputs, pytorch_ref
            
        except Exception as e:
            logger.error(f"æå–æµ‹è¯•ä¿¡æ¯å¤±è´¥: {e}")
            return None, None

    def _create_test_inputs_from_code(self, test_code: str) -> Optional[List[torch.Tensor]]:
        """
        ä»æµ‹è¯•ä»£ç ä¸­åˆ›å»ºæµ‹è¯•è¾“å…¥
        
        Args:
            test_code: æµ‹è¯•ä»£ç å­—ç¬¦ä¸²
            
        Returns:
            æµ‹è¯•è¾“å…¥å¼ é‡åˆ—è¡¨
        """
        try:
            # ç®€å•çš„æ¨¡å¼åŒ¹é…æ¥æå–å¼ é‡åˆ›å»º
            import re
            
            # æŸ¥æ‰¾torch.randn, torch.randç­‰è°ƒç”¨ï¼ŒåŒ…æ‹¬æ›´å¤æ‚çš„å‚æ•°æ¨¡å¼
            tensor_patterns = [
                r'torch\.randn\(([^)]+)\)',
                r'torch\.rand\(([^)]+)\)',
                r'torch\.zeros\(([^)]+)\)',
                r'torch\.ones\(([^)]+)\)',
                r'torch\.tensor\(([^)]+)\)'
            ]
            
            test_inputs = []
            detected_dtype = torch.float32  # é»˜è®¤ç±»å‹
            detected_device = self.device
            
            # é¦–å…ˆæ£€æµ‹æµ‹è¯•ä»£ç ä¸­ä½¿ç”¨çš„æ•°æ®ç±»å‹
            if 'torch.bfloat16' in test_code or 'dtype=torch.bfloat16' in test_code:
                detected_dtype = torch.bfloat16
            elif 'torch.float16' in test_code or 'dtype=torch.float16' in test_code:
                detected_dtype = torch.float16
            elif 'torch.float32' in test_code or 'dtype=torch.float32' in test_code:
                detected_dtype = torch.float32
            
            logger.info(f"æ£€æµ‹åˆ°æµ‹è¯•ä»£ç ä¸­çš„æ•°æ®ç±»å‹: {detected_dtype}")
            
            for pattern in tensor_patterns:
                matches = re.findall(pattern, test_code)
                for match in matches:
                    try:
                        # è§£æå‚æ•° - æ›´æ™ºèƒ½çš„è§£æ
                        args_str = match.strip()
                        
                        # æå–å½¢çŠ¶å‚æ•°
                        shape_args = []
                        dtype = detected_dtype  # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç±»å‹
                        device = detected_device
                        
                        # åˆ†å‰²å‚æ•°ï¼Œä½†è¦å¤„ç†åµŒå¥—çš„æƒ…å†µ
                        args = []
                        paren_count = 0
                        current_arg = ""
                        
                        for char in args_str + ",":
                            if char == "," and paren_count == 0:
                                if current_arg.strip():
                                    args.append(current_arg.strip())
                                current_arg = ""
                            else:
                                if char in "([":
                                    paren_count += 1
                                elif char in ")]":
                                    paren_count -= 1
                                current_arg += char
                        
                        for arg in args:
                            arg = arg.strip()
                            
                            # æ£€æŸ¥æ•°æ®ç±»å‹
                            if 'dtype=' in arg:
                                if 'bfloat16' in arg:
                                    dtype = torch.bfloat16
                                elif 'float16' in arg:
                                    dtype = torch.float16
                                elif 'float32' in arg:
                                    dtype = torch.float32
                            
                            # æ£€æŸ¥è®¾å¤‡
                            elif 'device=' in arg:
                                if 'cuda' in arg:
                                    device = 'cuda'
                                elif 'cpu' in arg:
                                    device = 'cpu'
                            
                            # æå–æ•°å­—å½¢çŠ¶å‚æ•°
                            elif arg.isdigit():
                                shape_args.append(int(arg))
                            
                            # å¤„ç†å˜é‡å½¢çŠ¶ï¼ˆå¦‚ N = 1024ï¼‰
                            elif arg in ['N'] and 'N = 1024' in test_code:
                                shape_args.append(1024)
                        
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å½¢çŠ¶ï¼Œå°è¯•ä»å¸¸è§æ¨¡å¼ä¸­æå–
                        if not shape_args:
                            # æŸ¥æ‰¾ N = æ•°å­— çš„æ¨¡å¼
                            n_match = re.search(r'N\s*=\s*(\d+)', test_code)
                            if n_match:
                                shape_args = [int(n_match.group(1))]
                            else:
                                # é»˜è®¤å½¢çŠ¶
                                shape_args = [1024]
                        
                        if shape_args:
                            if device == 'cuda' and torch.cuda.is_available():
                                tensor = torch.randn(shape_args, dtype=dtype, device='cuda')
                            else:
                                tensor = torch.randn(shape_args, dtype=dtype)
                            test_inputs.append(tensor)
                            logger.info(f"åˆ›å»ºæµ‹è¯•å¼ é‡: shape={shape_args}, dtype={dtype}, device={device}")
                            
                    except Exception as e:
                        logger.debug(f"è§£æå¼ é‡å‚æ•°å¤±è´¥: {e}")
                        continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ›å»ºé»˜è®¤è¾“å…¥ï¼ˆä½¿ç”¨æ£€æµ‹åˆ°çš„ç±»å‹ï¼‰
            if not test_inputs:
                logger.info(f"æœªæ‰¾åˆ°å¼ é‡åˆ›å»ºæ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤è¾“å…¥: dtype={detected_dtype}")
                if self.device == 'cuda':
                    test_inputs = [torch.randn(1024, dtype=detected_dtype, device='cuda')]
                else:
                    test_inputs = [torch.randn(1024, dtype=detected_dtype)]
            
            return test_inputs
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæµ‹è¯•è¾“å…¥å¤±è´¥: {e}")
            return None

    def _create_pytorch_reference_from_code(self, test_code: str, test_inputs: List[torch.Tensor]) -> Optional[Any]:
        """
        æ ¹æ®æµ‹è¯•ä»£ç æ¨æ–­PyTorchå‚è€ƒå®ç°
        
        Args:
            test_code: æµ‹è¯•ä»£ç 
            test_inputs: æµ‹è¯•è¾“å…¥
            
        Returns:
            PyTorchå‚è€ƒå‡½æ•°
        """
        try:
            # æ ¹æ®æµ‹è¯•ä»£ç ä¸­çš„æ“ä½œæ¨æ–­
            code_lower = test_code.lower()
            
            if 'relu' in code_lower:
                def pytorch_relu(*inputs):
                    # ç¡®ä¿è¾“å…¥å’Œè¾“å‡ºç±»å‹ä¸€è‡´
                    input_tensor = inputs[0]
                    result = torch.relu(input_tensor)
                    # ç¡®ä¿ç»“æœä¸è¾“å…¥æœ‰ç›¸åŒçš„dtypeå’Œdevice
                    return result.to(dtype=input_tensor.dtype, device=input_tensor.device)
                return pytorch_relu
                
            elif 'softmax' in code_lower:
                def pytorch_softmax(*inputs):
                    input_tensor = inputs[0]
                    result = torch.softmax(input_tensor, dim=-1)
                    return result.to(dtype=input_tensor.dtype, device=input_tensor.device)
                return pytorch_softmax
                
            elif 'sigmoid' in code_lower:
                def pytorch_sigmoid(*inputs):
                    input_tensor = inputs[0]
                    result = torch.sigmoid(input_tensor)
                    return result.to(dtype=input_tensor.dtype, device=input_tensor.device)
                return pytorch_sigmoid
                
            elif 'add' in code_lower and len(test_inputs) >= 2:
                def pytorch_add(*inputs):
                    result = torch.add(inputs[0], inputs[1])
                    return result.to(dtype=inputs[0].dtype, device=inputs[0].device)
                return pytorch_add
                
            elif 'matmul' in code_lower and len(test_inputs) >= 2:
                def pytorch_matmul(*inputs):
                    result = torch.matmul(inputs[0], inputs[1])
                    return result.to(dtype=inputs[0].dtype, device=inputs[0].device)
                return pytorch_matmul
            
            else:
                # é»˜è®¤è¿”å›è¾“å…¥ï¼ˆç”¨äºæµ‹è¯•ï¼‰
                def pytorch_identity(*inputs):
                    return inputs[0].clone()
                return pytorch_identity
                
        except Exception as e:
            logger.error(f"åˆ›å»ºPyTorchå‚è€ƒå¤±è´¥: {e}")
            return None

    def _run_benchmark(self, triton_func: Any, pytorch_func: Any, test_inputs: List[torch.Tensor]) -> Dict[str, Any]:
        """
        è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            triton_func: Tritonå†…æ ¸å‡½æ•°
            pytorch_func: PyTorchå‚è€ƒå‡½æ•°
            test_inputs: æµ‹è¯•è¾“å…¥
            
        Returns:
            æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        results = {
            "success": False,
            "pytorch_time_ms": None,
            "triton_time_ms": None,
            "speedup": None,
            "memory_usage_mb": None
        }
        
        try:
            # æµ‹è¯•PyTorchæ€§èƒ½
            pytorch_time = self._benchmark_pytorch(pytorch_func, test_inputs)
            
            # æµ‹è¯•Tritonæ€§èƒ½
            triton_time = self._benchmark_triton(triton_func, test_inputs)
            
            if pytorch_time is not None and triton_time is not None:
                speedup = pytorch_time / triton_time if triton_time > 0 else 0
                
                results.update({
                    "success": True,
                    "pytorch_time_ms": pytorch_time,
                    "triton_time_ms": triton_time,
                    "speedup": speedup
                })
                
                # # è®¡ç®—å†…å­˜ä½¿ç”¨
                # if self.device == 'cuda':
                #     memory_mb = sum(t.numel() * t.element_size() for t in test_inputs) / (1024 * 1024)
                #     results["memory_usage_mb"] = memory_mb
            
        except Exception as e:
            results["error"] = str(e)
            logger.error(f"åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        
        return results

    def _benchmark_pytorch(self, pytorch_func: Any, test_inputs: List[torch.Tensor]) -> Optional[float]:
        """PyTorchæ€§èƒ½æµ‹è¯•"""
        try:
            if self.device == "cuda":
                torch.cuda.synchronize()
            
            # é¢„çƒ­
            for _ in range(self.warmup_runs):
                with torch.no_grad():
                    pytorch_func(*test_inputs)
            
            if self.device == "cuda":
                torch.cuda.synchronize()
            
            # åŸºå‡†æµ‹è¯•
            start_time = time.perf_counter()
            for _ in range(self.benchmark_runs):
                with torch.no_grad():
                    pytorch_func(*test_inputs)
            
            if self.device == "cuda":
                torch.cuda.synchronize()
            
            end_time = time.perf_counter()
            avg_time = (end_time - start_time) / self.benchmark_runs * 1000
            return avg_time
            
        except Exception as e:
            logger.error(f"PyTorchåŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return None

    def _benchmark_triton(self, triton_func: Any, test_inputs: List[torch.Tensor]) -> Optional[float]:
        """Tritonæ€§èƒ½æµ‹è¯•"""
        try:
            if self.device == "cuda":
                torch.cuda.synchronize()
            
            # é¢„çƒ­
            for _ in range(self.warmup_runs):
                triton_func(*test_inputs)
            
            if self.device == "cuda":
                torch.cuda.synchronize()
            
            # åŸºå‡†æµ‹è¯•
            start_time = time.perf_counter()
            for _ in range(self.benchmark_runs):
                triton_func(*test_inputs)
            
            if self.device == "cuda":
                torch.cuda.synchronize()
            
            end_time = time.perf_counter()
            avg_time = (end_time - start_time) / self.benchmark_runs * 1000
            return avg_time
            
        except Exception as e:
            logger.error(f"TritonåŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return None

    def format_performance_summary(self, results: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æ€§èƒ½æµ‹è¯•ç»“æœæ‘˜è¦
        
        Args:
            results: æ€§èƒ½æµ‹è¯•ç»“æœ
            
        Returns:
            æ ¼å¼åŒ–çš„æ‘˜è¦å­—ç¬¦ä¸²
        """
        if not results["success"]:
            return f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {results.get('error', 'Unknown error')}"
        
        pytorch_time = results["pytorch_time_ms"]
        triton_time = results["triton_time_ms"]
        speedup = results["speedup"]
        
        
        summary = f"""ğŸš€ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:
   PyTorch: {pytorch_time:.3f}ms
   Triton:  {triton_time:.3f}ms
   åŠ é€Ÿæ¯”:  {speedup:.2f}x
   è¯„çº§:    {grade}"""
        
        if results.get("memory_usage_mb"):
            summary += f"\n   å†…å­˜:    {results['memory_usage_mb']:.1f}MB"
        
        return summary


def benchmark_successful_kernel(session_dir: str) -> Optional[Dict[str, Any]]:
    """
    å¯¹æˆåŠŸéªŒè¯çš„å†…æ ¸è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    
    Args:
        session_dir: ä¼šè¯ç›®å½•è·¯å¾„ï¼ŒåŒ…å«kernel.pyå’Œtest.py
        
    Returns:
        æ€§èƒ½æµ‹è¯•ç»“æœï¼Œå¤±è´¥è¿”å›None
    """
    try:
        session_path = Path(session_dir)
        kernel_file = session_path / "final_kernel.py"
        test_file = session_path / "test.py"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not kernel_file.exists():
            logger.warning(f"å†…æ ¸æ–‡ä»¶ä¸å­˜åœ¨: {kernel_file}")
            return None
            
        if not test_file.exists():
            logger.warning(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return None
        
        # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
        benchmark = TritonPerformanceBenchmark(warmup_runs=3, benchmark_runs=20)
        
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        results = benchmark.benchmark_kernel_from_file(str(kernel_file), str(test_file))
        
        # ä¿å­˜ç»“æœ
        if results["success"]:
            perf_file = session_path / "performance_results.json"
            with open(perf_file, 'w', encoding='utf-8') as f:
                import json
                json.dump(results, f, indent=2)
        
        return results
        
    except Exception as e:
        logger.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        return None