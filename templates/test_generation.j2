Generate a comprehensive test for a Triton kernel based on the following problem description:

{{ problem_description }}

{% if additional_code -%}
REFERENCE IMPLEMENTATION (for numerical intuition only – PyTorch remains the baseline target):
```python
{{ additional_code }}
```
{%- endif %}

{% if baseline_code -%}
BASELINE TRITON IMPLEMENTATION PROVIDED BY USER (context only, do NOT execute unless explicitly required):
```python
{{ baseline_code }}
```
{%- endif %}

{% if baseline_metrics %}
BASELINE PERFORMANCE TARGET:
- Baseline Triton speedup vs PyTorch: {{ "%.4f"|format(baseline_metrics.speedup) }}x (Triton {{ "%.4f"|format(baseline_metrics.triton_ms) }} ms vs PyTorch {{ "%.4f"|format(baseline_metrics.pytorch_ms) }} ms)
- Any candidate that does not EXCEED this speedup must fail, even if it beats PyTorch.
{% else %}
BASELINE PERFORMANCE TARGET:
- The agent will inject a `BASELINE_METRICS` dictionary at runtime containing `{"triton_ms": ..., "pytorch_ms": ..., "speedup": ...}` for the user-supplied baseline.
- Your test must fail whenever the candidate speedup is missing OR not strictly greater than `BASELINE_METRICS["speedup"]`.
{% endif %}

{% if provided_test_code -%}
REFERENCE TEST CODE:
```python
{{ provided_test_code }}
```
Analyze the reference but generate a new test that follows the updated structure below.
{%- endif %}

GENERAL REQUIREMENTS:
1. Produce a single, self-contained Python file.
2. Import the kernel via `from kernel import kernel_function` – never launch Triton manually.
3. Use EXACT shapes/dtypes stated in the problem. If FP32 is requested, use BF16 everywhere instead (inputs, expected results, computations).
4. Include rich debugging on mismatches (shapes, dtype, values, diffs, tolerances).
5. Wrap comparisons in try/except so failures produce actionable logs.
6. Exit with status 0 on success, 1 on failure.
7. Prevent time-doping: during benchmarking, clone or regenerate every input inside the timed functions so each call uses fresh storage; this defeats pointer-based caching tricks.

NUMERICAL REQUIREMENTS:
- Default tolerances: `rtol=1e-3`, `atol=1e-3`; relax only with clear justification and comments.
- Document any deviation from defaults.

PERFORMANCE & METRICS REQUIREMENTS:
- Benchmark both the Triton kernel and the PyTorch reference using `triton.testing.do_bench(fn, warmup=25, rep=100, return_mode="mean")`.
- After benchmarking, emit **exactly one** structured metrics line: `print("PERF_METRICS:" + json.dumps({"triton_ms": triton_ms, "pytorch_ms": pytorch_ms, "speedup": speedup}))`.
- Print a short human-readable summary as well, but the JSON line is mandatory.
- Fail the test if:
  * `speedup <= 1.0`, or
  * Baseline metrics are present and `speedup <= BASELINE_METRICS["speedup"]`.

STRUCTURE TO FOLLOW:
```python
import json
import torch
import triton
import triton.testing as tt

BASELINE_METRICS = None  # Agent will replace with dict when metrics are known.

# Summary comment of the original problem
def test_kernel() -> bool:
    try:
        from kernel import kernel_function
    except Exception as exc:
        print(f"Failed to import kernel_function: {exc}")
        return False

    try:
        if not torch.cuda.is_available():
            print("CUDA is not available. This test requires a CUDA-capable GPU.")
            return False

        current_dev = torch.cuda.current_device()
        device = torch.device(f"cuda:{current_dev}")
        torch.cuda.set_device(device)
        torch.manual_seed(0)

        # 1. PREPARE INPUTS EXACTLY AS SPECIFIED
        # x = torch.randn(..., device=device, dtype=torch.bfloat16)
        # ... additional tensors as needed ...

        # 2. COMPUTE EXPECTED RESULT WITH PYTORCH
        # expected = reference_impl(x, ...)

        # 3. RUN KERNEL (HANDLE RETURN / OUT PARAMETER PATTERNS)
        # result = kernel_function(x, ...)

        # 4. VERIFY CORRECTNESS WITH DETAILED DEBUGGING
        # if not torch.allclose(result, expected, rtol=1e-3, atol=1e-3):
        #     print("NUMERICAL MISMATCH:")
        #     ...
        #     return False

        # 5. BENCHMARK
        def triton_run():
            # Clone ALL inputs here so each call has fresh storage (prevents caching cheats).
            return kernel_function(x.clone())  # replace with clones for every argument the kernel expects

        def torch_run():
            # Mirror the cloning pattern for the PyTorch reference to keep comparisons fair.
            return torch.relu(x.clone())  # replace with correct PyTorch op and cloned inputs

        _ = triton_run(); _ = torch_run(); torch.cuda.synchronize()
        triton_ms = tt.do_bench(triton_run, warmup=25, rep=100, return_mode="mean")
        pytorch_ms = tt.do_bench(torch_run, warmup=25, rep=100, return_mode="mean")
        speedup = pytorch_ms / triton_ms if triton_ms > 0 else float("inf")

        metrics = {
            "triton_ms": float(triton_ms),
            "pytorch_ms": float(pytorch_ms),
            "speedup": float(speedup),
        }
        print("Performance Results:")
        print(f"  Triton kernel: {triton_ms:.4f} ms")
        print(f"  PyTorch native: {pytorch_ms:.4f} ms")
        print(f"  Speedup: {speedup:.2f}x")
        print("PERF_METRICS:" + json.dumps(metrics))

        if speedup <= 1.0:
            print("PERFORMANCE FAILURE: Triton kernel is not faster than PyTorch.")
            return False

        if BASELINE_METRICS is not None and speedup <= BASELINE_METRICS.get("speedup", float("inf")):
            print(
                "PERFORMANCE FAILURE: candidate speedup "
                f"{speedup:.4f}x <= baseline {BASELINE_METRICS.get('speedup'):.4f}x"
            )
            return False

        return True

    except Exception as exc:
        print(f"Test failed due to unexpected exception: {exc}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import sys
    sys.exit(0 if test_kernel() else 1)
```

Generate the full test implementation following the template above, adapting shapes, dtypes, and operations to the supplied problem (and any contextual code) while respecting all correctness and performance gates.
